{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using the <code>glob</code> library allows us to create a list with only the csv that are ended in <em>(Normalized).csv</em> , which will be the most useful for a statistical analysis. <br>\n",
    "Moreover, for a future simplification, we also create a list with the relevant information of each of the csv <em>('Area','Year','Element','Unit','Value')</em> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\msanchis\\\\Documents\\\\GitHub\\\\python-data-driven-decisions/Data\\\\Value_of_Production_E_All_Data_(Normalized).csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(os.getcwd()+ \"/Data/\"+\"/**(Normalized).csv\")\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intitiate the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our First Spark example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standarization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we will compare all the csv to see which are not up to our standards of provided data (columns should be as followed: *Area , Year, Element , Item , Unit* and *Value*). Firstly we will collect the number inside the `file_list` of the files which are not up to our standars into a list called missing, which later on will be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area=[]\n",
    "Year=[]\n",
    "Element=[]\n",
    "Item=[]\n",
    "Unit=[]\n",
    "Value=[]\n",
    "for i in range (0,len(file_list)):\n",
    "    file1=ss.read.csv(file_list[i], header=True)\n",
    "    if 'Area' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Area.append(i)\n",
    "        continue\n",
    "    if 'Year' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Year.append(i)\n",
    "    if 'Element' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Element.append(i)\n",
    "    if 'Item' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Item.append(i)\n",
    "    if 'Unit' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Unit.append(i)\n",
    "    if 'Value' in file1.schema.fieldNames():\n",
    "        pass\n",
    "    else:\n",
    "        Value.append(i)\n",
    "missing=Area + Year+ Element + Item + Unit +Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the obtained list, we create a loop to visualy inspect each file which does not met the standars so we can observe how we can adapt them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+----------+----------+----------------------+-----------------+---------+----------+------------+---------+------------+----------------+---------+----+--------+---------+----+----+\n",
      "|Donor Code|Donor     |Recipient Country Code|Recipient Country|Item Code|Item      |Element Code|Element  |Purpose Code|Purpose         |Year Code|Year|Unit    |Value    |Flag|Note|\n",
      "+----------+----------+----------------------+-----------------+---------+----------+------------+---------+------------+----------------+---------+----+--------+---------+----+----+\n",
      "|702       |All Donors|2                     |Afghanistan      |22040    |Commitment|6110        |Value US$|10000       |All CRS purposes|1973     |1973|millions|41.627000|Fc  |null|\n",
      "|702       |All Donors|2                     |Afghanistan      |22040    |Commitment|6110        |Value US$|10000       |All CRS purposes|1974     |1974|millions|40.150527|Fc  |null|\n",
      "|702       |All Donors|2                     |Afghanistan      |22040    |Commitment|6110        |Value US$|10000       |All CRS purposes|1975     |1975|millions|32.113282|Fc  |null|\n",
      "|702       |All Donors|2                     |Afghanistan      |22040    |Commitment|6110        |Value US$|10000       |All CRS purposes|1976     |1976|millions|89.795781|Fc  |null|\n",
      "|702       |All Donors|2                     |Afghanistan      |22040    |Commitment|6110        |Value US$|10000       |All CRS purposes|1977     |1977|millions|70.978592|Fc  |null|\n",
      "+----------+----------+----------------------+-----------------+---------+----------+------------+---------+------------+----------------+---------+----+--------+---------+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "37\n",
      "+----------------------+-----------------+---------+---------------+------------+-----------------+---------+----+------+-----------+----+--------------------+\n",
      "|Recipient Country Code|Recipient Country|Item Code|Item           |Element Code|Element          |Year Code|Year|Unit  |Value      |Flag|Note                |\n",
      "+----------------------+-----------------+---------+---------------+------------+-----------------+---------+----+------+-----------+----+--------------------+\n",
      "|2                     |Afghanistan      |10109    |Blended And Mix|500         |Food aid received|1990     |1990|tonnes|1992.000000|X   |Data provided by WFP|\n",
      "|2                     |Afghanistan      |10109    |Blended And Mix|500         |Food aid received|1999     |1999|tonnes|4482.000000|X   |Data provided by WFP|\n",
      "|2                     |Afghanistan      |10109    |Blended And Mix|500         |Food aid received|2000     |2000|tonnes|1053.000000|X   |Data provided by WFP|\n",
      "|2                     |Afghanistan      |10109    |Blended And Mix|500         |Food aid received|2001     |2001|tonnes|5518.000000|X   |Data provided by WFP|\n",
      "|2                     |Afghanistan      |10109    |Blended And Mix|500         |Food aid received|2002     |2002|tonnes|8517.000000|X   |Data provided by WFP|\n",
      "+----------------------+-----------------+---------+---------------+------------+-----------------+---------+----+------+-----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "40\n",
      "+---------------------------+---------------------------+------------------+--------------------------+--------------------------+-----------------+---------------+-------------------------------+------------------+---------------+---------------+----+--------+---------+----+\n",
      "|Reporter Country Code (FAO)|Reporter Country Code (M49)|Reporter Countries|Partner Country Code (FAO)|Partner Country Code (M49)|Partner Countries|Item Code (FAO)|Item                           |Element Code (FAO)|Element        |Year Code (FAO)|Year|Unit    |Value    |Flag|\n",
      "+---------------------------+---------------------------+------------------+--------------------------+--------------------------+-----------------+---------------+-------------------------------+------------------+---------------+---------------+----+--------+---------+----+\n",
      "|2                          |2                          |Afghanistan       |10                        |10                        |Australia        |1877           |Forest products (export/import)|5622              |Import Value   |1999           |1999|1000 US$|6.000000 |null|\n",
      "|2                          |2                          |Afghanistan       |10                        |10                        |Australia        |1877           |Forest products (export/import)|5622              |Import Value   |2005           |2005|1000 US$|18.000000|null|\n",
      "|2                          |2                          |Afghanistan       |10                        |10                        |Australia        |1877           |Forest products (export/import)|5622              |Import Value   |2009           |2009|1000 US$|59.000000|null|\n",
      "|2                          |2                          |Afghanistan       |10                        |10                        |Australia        |1877           |Forest products (export/import)|5622              |Import Value   |2015           |2015|1000 US$|5.000000 |null|\n",
      "|2                          |2                          |Afghanistan       |10                        |10                        |Australia        |1640           |Plywood                        |5616              |Import Quantity|1999           |1999|m3      |5.000000 |R   |\n",
      "+---------------------------+---------------------------+------------------+--------------------------+--------------------------+-----------------+---------------+-------------------------------+------------------+---------------+---------------+----+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "41\n",
      "+-----------+--------------+-----------------------+------------------+------------------------------------------+-------------------------------------+--------------+-----------------------------------+------------+-------------------+--------------+-----------+----+\n",
      "|Survey Code|Survey        |Breakdown Variable Code|Breakdown Variable|Breadown by Sex of the Household Head Code|Breadown by Sex of the Household Head|Indicator Code|Indicator                          |Measure Code|Measure            |Unit          |Value      |Flag|\n",
      "+-----------+--------------+-----------------------+------------------+------------------------------------------+-------------------------------------+--------------+-----------------------------------+------------+-------------------+--------------+-----------+----+\n",
      "|32005      |Albania - 2005|20008                  |Country-level     |20002                                     |Male-headed household                |6061          |Total consumption in monetary value|6076        |Mean               |LCU/person/day|6617.690000|CS  |\n",
      "|32005      |Albania - 2005|20008                  |Country-level     |20002                                     |Male-headed household                |6061          |Total consumption in monetary value|6077        |Median             |LCU/person/day|5541.070000|CS  |\n",
      "|32005      |Albania - 2005|20008                  |Country-level     |20002                                     |Male-headed household                |6061          |Total consumption in monetary value|6078        |Standard Deviation |LCU/person/day|4375.790000|CS  |\n",
      "|32005      |Albania - 2005|20008                  |Country-level     |20002                                     |Male-headed household                |6061          |Total consumption in monetary value|6079        |Number Observations|LCU/person/day|3445.000000|CS  |\n",
      "|32005      |Albania - 2005|20008                  |Country-level     |20002                                     |Male-headed household                |6062          |Food consumption in monetary value |6076        |Mean               |LCU/person/day|1704.400000|CS  |\n",
      "+-----------+--------------+-----------------------+------------------+------------------------------------------+-------------------------------------+--------------+-----------------------------------+------------+-------------------+--------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "64\n",
      "+---------------------+------------------+--------------------+-----------------+---------+--------------------+------------+---------------+---------+----+--------+---------+----+\n",
      "|Reporter Country Code|Reporter Countries|Partner Country Code|Partner Countries|Item Code|Item                |Element Code|Element        |Year Code|Year|Unit    |Value    |Flag|\n",
      "+---------------------+------------------+--------------------+-----------------+---------+--------------------+------------+---------------+---------+----+--------+---------+----+\n",
      "|2                    |Afghanistan       |4                   |Algeria          |230      |Cashew nuts, shelled|5910        |Export Quantity|2016     |2016|tonnes  |3.000000 |*   |\n",
      "|2                    |Afghanistan       |4                   |Algeria          |230      |Cashew nuts, shelled|5922        |Export Value   |2016     |2016|1000 US$|23.000000|*   |\n",
      "|2                    |Afghanistan       |4                   |Algeria          |1293     |Crude materials     |5922        |Export Value   |2015     |2015|1000 US$|1.000000 |*   |\n",
      "|2                    |Afghanistan       |4                   |Algeria          |1293     |Crude materials     |5922        |Export Value   |2016     |2016|1000 US$|1.000000 |*   |\n",
      "|2                    |Afghanistan       |4                   |Algeria          |1293     |Crude materials     |5922        |Export Value   |2017     |2017|1000 US$|null     |null|\n",
      "+---------------------+------------------+--------------------+-----------------+---------+--------------------+------------+---------------+---------+----+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "67\n",
      "+---------+-----------+---------+--------+------------+------------+--------------+---------+----------------+-----------+----+--------------+----+----+\n",
      "|Area Code|Area       |Item Code|Item    |Element Code|Element     |WCA Round code|WCA Round|Census Year Code|Census Year|Unit|Value         |Flag|Note|\n",
      "+---------+-----------+---------+--------+------------+------------+--------------+---------+----------------+-----------+----+--------------+----+----+\n",
      "|2        |Afghanistan|27002    |Holdings|60850       |Number      |2000          |2000     |2003            |2003       |No  |3044670.000000|null|null|\n",
      "|3        |Albania    |27002    |Holdings|50260       |Area        |2000          |2000     |1998            |1998       |ha  |1889498.000000|null|null|\n",
      "|3        |Albania    |27002    |Holdings|50260       |Area        |2010          |2010     |2012            |2012       |ha  |371609.000000 |null|null|\n",
      "|3        |Albania    |27002    |Holdings|5017        |Average area|2000          |2000     |1998            |1998       |ha  |4.050000      |Fc  |null|\n",
      "|3        |Albania    |27002    |Holdings|5017        |Average area|2010          |2010     |2012            |2012       |ha  |1.160000      |Fc  |null|\n",
      "+---------+-----------+---------+--------+------------+------------+--------------+---------+----------------+-----------+----+--------------+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "3\n",
      "+---------+-----------+---------+------------------------------------------+-----------+-------+---------+----+----+---------+----+-----------------+\n",
      "|Area Code|Area       |Item Code|Item                                      |Months Code|Months |Year Code|Year|Unit|Value    |Flag|Note             |\n",
      "+---------+-----------+---------+------------------------------------------+-----------+-------+---------+----+----+---------+----+-----------------+\n",
      "|2        |Afghanistan|23013    |Consumer Prices, Food Indices (2015 = 100)|7001       |January|2000     |2000|null|24.356332|F   |base year is 2015|\n",
      "|2        |Afghanistan|23013    |Consumer Prices, Food Indices (2015 = 100)|7001       |January|2001     |2001|null|29.944592|F   |base year is 2015|\n",
      "|2        |Afghanistan|23013    |Consumer Prices, Food Indices (2015 = 100)|7001       |January|2002     |2002|null|33.421952|F   |base year is 2015|\n",
      "|2        |Afghanistan|23013    |Consumer Prices, Food Indices (2015 = 100)|7001       |January|2003     |2003|null|39.967661|F   |base year is 2015|\n",
      "|2        |Afghanistan|23013    |Consumer Prices, Food Indices (2015 = 100)|7001       |January|2004     |2004|null|43.401939|F   |base year is 2015|\n",
      "+---------+-----------+---------+------------------------------------------+-----------+-------+---------+----+----+---------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "20\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                                                          |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2008     |2008|1000 persons|4426.731000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2012     |2012|1000 persons|2477.079000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2014     |2014|1000 persons|2824.353000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2017     |2017|1000 persons|2740.235000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|23      |Male |2008     |2008|1000 persons|2476.535000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "21\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                            |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2008     |2008|1000 persons|4426.731000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2012     |2012|1000 persons|2477.079000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2014     |2014|1000 persons|2824.353000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2017     |2017|1000 persons|2740.235000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|23      |Male |2008     |2008|1000 persons|2476.535000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "22\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                                  |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2008     |2008|1000 persons|6253.794000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2014     |2014|1000 persons|5500.374000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2017     |2017|1000 persons|4966.018000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|23      |Male |2008     |2008|1000 persons|3923.265000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|23      |Male |2014     |2014|1000 persons|4066.614000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "34\n",
      "+---------+-----------+---------+-------------------------------------+-----------------+--------+---------+----+----+--------------+----+------------------+\n",
      "|Area Code|Area       |Item Code|Item                                 |ISO Currency Code|Currency|Year Code|Year|Unit|Value         |Flag|Note              |\n",
      "+---------+-----------+---------+-------------------------------------+-----------------+--------+---------+----+----+--------------+----+------------------+\n",
      "|2        |Afghanistan|5540     |Standard local currency units per USD|AFN              |Afghani |1970     |1970|null|0.044998427080|X   |Data from UNSD AMA|\n",
      "|2        |Afghanistan|5540     |Standard local currency units per USD|AFN              |Afghani |1971     |1971|null|0.044998427080|X   |Data from UNSD AMA|\n",
      "|2        |Afghanistan|5540     |Standard local currency units per USD|AFN              |Afghani |1972     |1972|null|0.044998427080|X   |Data from UNSD AMA|\n",
      "|2        |Afghanistan|5540     |Standard local currency units per USD|AFN              |Afghani |1973     |1973|null|0.044998427080|X   |Data from UNSD AMA|\n",
      "|2        |Afghanistan|5540     |Standard local currency units per USD|AFN              |Afghani |1974     |1974|null|0.044998427080|X   |Data from UNSD AMA|\n",
      "+---------+-----------+---------+-------------------------------------+-----------------+--------+---------+----+----+--------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "20\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                                                          |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2008     |2008|1000 persons|4426.731000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2012     |2012|1000 persons|2477.079000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2014     |2014|1000 persons|2824.353000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|13      |Total|2017     |2017|1000 persons|2740.235000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture, forestry and fishing by age, total (15+)|23      |Male |2008     |2008|1000 persons|2476.535000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "21\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                            |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2008     |2008|1000 persons|4426.731000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2012     |2012|1000 persons|2477.079000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2014     |2014|1000 persons|2824.353000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|13      |Total|2017     |2017|1000 persons|2740.235000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21066         |Employment in agriculture by age, 15+|23      |Male |2008     |2008|1000 persons|2476.535000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "22\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|Area Code|Area       |Source Code|Source                                 |Indicator Code|Indicator                                  |Sex Code|Sex  |Year Code|Year|Unit        |Value      |Flag|Note                                                                                                   |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2008     |2008|1000 persons|6253.794000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2014     |2014|1000 persons|5500.374000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|13      |Total|2017     |2017|1000 persons|4966.018000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|23      |Male |2008     |2008|1000 persons|3923.265000|X   |Break in series Break in series: Methodology revised Repository: ILO-STATISTICS - Micro data processing|\n",
      "|2        |Afghanistan|3021       |Household income and expenditure survey|21087         |Employment by age, total (15+), rural areas|23      |Male |2014     |2014|1000 persons|4066.614000|X   |Repository: ILO-STATISTICS - Micro data processing                                                     |\n",
      "+---------+-----------+-----------+---------------------------------------+--------------+-------------------------------------------+--------+-----+---------+----+------------+-----------+----+-------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "32\n",
      "+---------+-----------+-----------+-------+------------+------------------+---------+----+----+---------+----+\n",
      "|Area Code|Area       |Months Code|Months |Element Code|Element           |Year Code|Year|Unit|Value    |Flag|\n",
      "+---------+-----------+-----------+-------+------------+------------------+---------+----+----+---------+----+\n",
      "|2        |Afghanistan|7001       |January|7271        |Temperature change|1961     |1961|�C  |0.786000 |Fc  |\n",
      "|2        |Afghanistan|7001       |January|7271        |Temperature change|1962     |1962|�C  |0.039000 |Fc  |\n",
      "|2        |Afghanistan|7001       |January|7271        |Temperature change|1963     |1963|�C  |2.718000 |Fc  |\n",
      "|2        |Afghanistan|7001       |January|7271        |Temperature change|1964     |1964|�C  |-5.226000|Fc  |\n",
      "|2        |Afghanistan|7001       |January|7271        |Temperature change|1965     |1965|�C  |1.889000 |Fc  |\n",
      "+---------+-----------+-----------+-------+------------+------------------+---------+----+----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in missing:\n",
    "    print(i)\n",
    "    fileS=ss.read.csv(file_list[i], sep=',',header= True).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the results, we can reason the following:\n",
    "- For file number **3**, the missing value is *Element*, and as we cannot substitute, it will be replaced by a 0, thanks to the `lit` clause from the `pyspark` library.\n",
    "- In file **34**, we have the same problems as number 3 for *Element* and the units appear in the column called *Currency* therefore a substitution will be needed.\n",
    "- For files **20,21** and **22** the columns to be substitiuted are *Indicator* and *Sex* by *Item* and *Element*.\n",
    "- Whereas for **32** we have a similar case as for number 3 where *Item* cannot be substitude and will be replaced by 0.\n",
    "- In file 5 and **37**, the missing values is *Area*, however it can be substituted by *Recipient Country*, which will be made later on. \n",
    "- For files **40** and **64**, the *Area* is missing, however, we can concat the columns *Reporter Countries* and *Partner Countries*.\n",
    "- In file **41** \n",
    "- And finally for file *67*, we will adapt the *Census year* into *Year*.\n",
    "\n",
    "In the following cell, we create some list of the files that have things in common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=[5,37]\n",
    "C=[20,21,22]\n",
    "F=[40,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we create the Raw_Normalized data base by applying all the steps mentioned before through the `pyspark` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file0 = ss.read.csv(file_list[0], sep=',',header= True)\n",
    "file0=file0.select('Area','Year', (concat(col('Item'),lit(' - '), col('Element'),lit(' - '),col('Unit'))).alias('Unique'),'Value')\n",
    "for i in range (1,len(file_list)):\n",
    "    file1=ss.read.csv(file_list[i], header=True)\n",
    "    if i in missing:\n",
    "        if i == 3:\n",
    "          file1=file1.withColumn('Element',lit(0))          \n",
    "        elif i in B:\n",
    "          file1=file1.withColumn('Area',col('Recipient Country'))          \n",
    "        elif i in C:\n",
    "          file1=file1.withColumn('Element',col('Sex'))\n",
    "          file1=file1.withColumn('Item',col('Indicator'))          \n",
    "        elif i==32:\n",
    "          file1=file1.withColumn('Item',lit(0))          \n",
    "        elif i==34:\n",
    "          file1=file1.withColumn('Element',lit(0))\n",
    "          file1=file1.drop('Unit')\n",
    "          file1=file1.withColumn('Unit',col('Currency'))          \n",
    "        elif i in F:\n",
    "          file1=file1.withColumn( 'Area', concat(col(\"Reporter Countries\"), lit(\" - \"), col(\"Partner Countries\")))          \n",
    "        elif i==41:\n",
    "          file1=file1.withColumn( 'Element', concat(col(\"Breadown by Sex of the Household Head\"), lit(\" - \"), col(\"Measure\")))\n",
    "          file1=file1.withColumn('Item',col('Indicator'))\n",
    "          file1=file1.withColumn(\"Area\", split(col(\"Survey\"), \" - \").getItem(0)).withColumn(\"Year\", split(col(\"Survey\"), \" - \").getItem(1))         \n",
    "        elif i==67:\n",
    "          file1=file1.withColumn('Year',col('Census Year'))          \n",
    "    else:\n",
    "      pass\n",
    "    file1=file1.select('Area','Year', (concat(col('Item'),lit(' - '), col('Element'),lit(' - '),col('Unit'))).alias('Unique'),'Value')\n",
    "    file0=file0.union(file1)\n",
    "Raw_Normalized_FAODATA=file0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store data to Parquet Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'APPDATA': 'C:\\\\Users\\\\msanchis\\\\AppData\\\\Roaming',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'LES009743',\n",
       "        'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'FPS_BROWSER_APP_PROFILE_STRING': 'Internet Explorer',\n",
       "        'FPS_BROWSER_USER_PROFILE_STRING': 'Default',\n",
       "        'GPPT_CACHEFOLDER': 'C:\\\\Windows\\\\ccmcache',\n",
       "        'GPPT_INSTALLFOLDER': 'c:\\\\Installers',\n",
       "        'GPPT_LOGFOLDER': 'C:\\\\SCCM_logs',\n",
       "        'HADOOP_HOME': 'C:\\\\Program Files\\\\hadoop-3.2.0',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\msanchis',\n",
       "        'JAVA_HOME': 'C:\\\\Program Files\\\\Eclipse Foundation\\\\jdk-11.0.12.7-hotspot',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\msanchis\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\WADFRPAR28',\n",
       "        'NUMBER_OF_PROCESSORS': '8',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'C:\\\\Users\\\\msanchis\\\\Documents\\\\GitHub\\\\python-data-driven-decisions\\\\.venv\\\\Scripts;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files\\\\Eclipse Foundation\\\\jdk-11.0.12.7-hotspot\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;%HADOOP_HOME%\\\\bin;%JAVA_HOME%\\\\bin;C:\\\\WINDOWS\\\\system32\\\\config\\\\systemprofile\\\\.poetry\\\\bin;C:\\\\Users\\\\msanchis\\\\.poetry\\\\bin;C:\\\\Users\\\\msanchis\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\msanchis\\\\Documents\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\msanchis\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 142 Stepping 12, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': '8e0c',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PROMPT': '$P$G',\n",
       "        'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'SESSIONNAME': 'Console',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\WINDOWS',\n",
       "        'TEMP': 'C:\\\\Users\\\\msanchis\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\msanchis\\\\AppData\\\\Local\\\\Temp',\n",
       "        'UATDATA': 'C:\\\\WINDOWS\\\\CCM\\\\UATData\\\\D9F8C395-CAB8-491d-B8AC-179A1FE1BE77',\n",
       "        'USERDNSDOMAIN': 'CORP.CAPGEMINI.COM',\n",
       "        'USERDOMAIN': 'CORP',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'CORP',\n",
       "        'USERNAME': 'msanchis',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\msanchis',\n",
       "        'WINDIR': 'C:\\\\WINDOWS',\n",
       "        'PLAT': 'win-amd64',\n",
       "        'VIRTUAL_ENV': 'C:\\\\Users\\\\msanchis\\\\Documents\\\\GitHub\\\\python-data-driven-decisions\\\\.venv',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_INTERRUPT_EVENT': '1500',\n",
       "        'IPY_INTERRUPT_EVENT': '1500',\n",
       "        'JPY_PARENT_PID': '2308',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'SPARK_AUTH_SOCKET_TIMEOUT': '15',\n",
       "        'SPARK_BUFFER_SIZE': '65536'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------------+------------+\n",
      "|       Area|Year|             Item|       Value|\n",
      "+-----------+----+-----------------+------------+\n",
      "|Afghanistan|1976|Almonds, in shell|49550.000000|\n",
      "|Afghanistan|1977|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1978|Almonds, in shell|60673.000000|\n",
      "|Afghanistan|1979|Almonds, in shell|53089.000000|\n",
      "|Afghanistan|1980|Almonds, in shell|50055.000000|\n",
      "|Afghanistan|1981|Almonds, in shell|40449.000000|\n",
      "|Afghanistan|1982|Almonds, in shell|55617.000000|\n",
      "|Afghanistan|1983|Almonds, in shell|49044.000000|\n",
      "|Afghanistan|1984|Almonds, in shell|53089.000000|\n",
      "|Afghanistan|1985|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1986|Almonds, in shell|50561.000000|\n",
      "|Afghanistan|1987|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1988|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1989|Almonds, in shell|44494.000000|\n",
      "|Afghanistan|1990|Almonds, in shell|48033.000000|\n",
      "|Afghanistan|1991|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1992|Almonds, in shell|50055.000000|\n",
      "|Afghanistan|1993|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1994|Almonds, in shell|45505.000000|\n",
      "|Afghanistan|1995|Almonds, in shell|45505.000000|\n",
      "+-----------+----+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o120.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m file0\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      3\u001b[0m file0\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mfile0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataRaw.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\python-data-driven-decisions\\.venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\python-data-driven-decisions\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\python-data-driven-decisions\\.venv\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\python-data-driven-decisions\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o120.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n"
     ]
    }
   ],
   "source": [
    "file0 = ss.read.csv(file_list[0], sep=',',header= True).select(\"Area\", \"Year\", \"Item\", \"Value\")\n",
    "file0.show()\n",
    "file0.printSchema()\n",
    "file0.write.parquet(\"DataRaw.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration through pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at the beginning we do not have enough computer power to process the whole database, we are going to develop a test run to be sure that the ideas are escalable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first test run for the loop, we are going to load and process the data into concated data frames, where later on, there is an application of the <code>pivot_table</code> function which adjusts all the variables, previouly called 'Elements' & 'Units' into the headers of the columns, and the values the values of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframe = pd.DataFrame(pd.read_csv(file_list[0], sep=',', encoding='latin-1'),columns=CC)\n",
    "for i in range(1,5):\n",
    "    df = pd.DataFrame(pd.read_csv(file_list[i],sep=',' , encoding='latin-1',low_memory=False), columns=CC)\n",
    "    main_dataframe = pd.concat([main_dataframe, df])\n",
    "\n",
    "main_dataframeC=main_dataframe.pivot_table(index=['Area','Year'], columns= ['Element' or 'Item','Unit'], values='Value')\n",
    "main_dataframeC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the following cell, we are concatanating all the files from the <code>file_list</code>, which will have the same shape thanks to creation of the dataframes with the restriction of the columns. <br>\n",
    "Moreover, this concat function will allow for a single data frame which has all the files one on top of another. Therefore the final result form this loop will be <code>main_dataframe</code> which will be our Normalized Source Data Model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframe = pd.DataFrame(pd.read_csv(file_list[0], sep=',', encoding='latin-1'),columns=CC)\n",
    "for i in range(1,len(file_list)):\n",
    "    df = pd.DataFrame(pd.read_csv(file_list[i],sep=',' , encoding='latin-1',low_memory=False), columns=CC)\n",
    "    main_dataframe = pd.concat([main_dataframe, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lastly, to convert the Normalized Source Data Model into the Normalized Integrated Data Model, we are going to use the <code>pivot_table</code> function which allows to <br>\n",
    "adjusts all the variables, previouly called <em>'Elements' & 'Units'</em> into the headers of the columns, and the <em>'Value'</em> column will be the values of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataframeC=main_dataframe.pivot_table(index=['Area','Year'], columns= ['Element' or 'Item' ,'Unit'], values='Value')\n",
    "main_dataframeC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are going to make sure that none of our interesting variables from <em>'Elements'</em> have been left out, thus checking if the extraction & integtration has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction & integration is COMPLETED and CORRECT\n"
     ]
    }
   ],
   "source": [
    "if len(main_dataframe[\"Element\" or 'Item'].value_counts())==main_dataframeC.shape[1]:\n",
    "    print('Data extraction & integration is COMPLETED and CORRECT')\n",
    "else:\n",
    "    print('Data extraction & integration is UNCOMPLETED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0581013962d1422e924d6baa4c1347697ec037e3370ec501496e00dda3f2654b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
